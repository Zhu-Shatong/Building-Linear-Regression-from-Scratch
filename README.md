# Building-Linear-Regression-from-Scratch

This project is not about using ready-made libraries; it's an exploration into the core principles that power linear regression. We start from basic mathematics and progressively build up to a fully functioning linear regression model. This hands-on approach is designed for learners and enthusiasts who want to deeply understand the intricacies of one of the most fundamental algorithms in machine learning. Dive in to experience linear regression like never before!

è¿™ä¸ªé¡¹ç›®ä¸æ˜¯å…³äºŽä½¿ç”¨çŽ°æˆçš„åº“ï¼Œè€Œæ˜¯å¯¹é©±åŠ¨çº¿æ€§å›žå½’çš„æ ¸å¿ƒåŽŸåˆ™çš„ä¸€æ¬¡æŽ¢ç´¢ã€‚æˆ‘ä»¬ä»ŽåŸºç¡€æ•°å­¦å¼€å§‹ï¼Œé€æ­¥æž„å»ºå‡ºä¸€ä¸ªåŠŸèƒ½å®Œå–„çš„çº¿æ€§å›žå½’æ¨¡åž‹ã€‚è¿™ç§å®žè·µæ–¹æ³•ä¸“ä¸ºé‚£äº›å¸Œæœ›æ·±å…¥ç†è§£æœºå™¨å­¦ä¹ ä¸­æœ€åŸºæœ¬ç®—æ³•ä¹‹ä¸€çš„å¤æ‚æ€§çš„å­¦ä¹ è€…å’Œçˆ±å¥½è€…è®¾è®¡ã€‚æ·±å…¥ä½“éªŒå‰æ‰€æœªæœ‰çš„çº¿æ€§å›žå½’ï¼

## ðŸŒŸ æ”¯æŒæˆ‘ä»¬ï¼

å¦‚æžœä½ å¯¹é¡¹ç›®æ„Ÿå…´è¶£ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼ðŸŒŸ

ä½ çš„æ”¯æŒæ˜¯æˆ‘ä»¬ä¸æ–­è¿›æ­¥å’Œåˆ›æ–°çš„æœ€å¤§åŠ¨åŠ›ï¼

[![Stargazers repo roster for @Zhu-Shatong/Building-Linear-Regression-from-Scratch](https://reporoster.com/stars/dark/Zhu-Shatong/Building-Linear-Regression-from-Scratch)](https://github.com/Zhu-Shatong/Building-Linear-Regression-from-Scratch/stargazers)



# çº¿æ€§å›žå½’ Linear Regression

CopyRight: Zhu Shatong , Tongji University
æœ¬notebookæ‰€æœ‰ç®—æ³•å‡ä¸ºæ‰‹å†™ï¼Œä¸ä½¿ç”¨ä»»ä½•åº“å‡½æ•°ã€‚

ï¼ˆç®—æ³•è®¾è®¡éƒ¨åˆ†ï¼‰ç›®å½•ï¼š
1. å‡†å¤‡å·¥ä½œï¼šæ•°æ®çš„å¯¼å…¥ä¸Žç›¸å…³é¢„å¤„ç†ï¼Œç›¸å…³å·¥å…·å‡½æ•°çš„å®šä¹‰
2. ï¼ˆå•å˜é‡çº¿æ€§å›žå½’çš„ï¼‰æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• Batch Gradient Descent
3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• Mini Batch Gradient Descentï¼ˆåœ¨æ‰¹é‡æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼‰
4. è‡ªé€‚åº”æ¢¯åº¦ä¸‹é™æ³• Adagradï¼ˆåœ¨å­¦ä¹ çŽ‡æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼‰
5. å¤šå˜é‡çº¿æ€§å›žå½’ Multivariate Linear Regressionï¼ˆåœ¨ç‰¹å¾æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œæ‹“å±•åˆ°å¤šä¸ªç‰¹å¾ï¼‰
6. L1æ­£åˆ™åŒ– L1 Regularizationï¼ˆä¹Ÿå°±æ˜¯Lasso Regressionï¼Œåº”å¯¹å¤šå˜é‡çš„è¿‡æ‹Ÿåˆï¼‰





## 0. æ•°æ®çš„å¯¼å…¥ä¸Žç›¸å…³é¢„å¤„ç†

åœ¨è¿™ä¸€section, æˆ‘ä»¬å°†ä¼šè´Ÿè´£å¯¼å…¥æ•°æ®ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œä¸€äº›é¢„å¤„ç†ï¼Œä»¥ä¾¿äºŽåŽç»­çš„æ“ä½œã€‚

dataï¼š

æˆ‘ä»¬é¦–å…ˆå¯¼å…¥çš„æ–‡ä»¶ä¸º `data.xlsx` ï¼Œå°†å®ƒå­˜å‚¨åœ¨dataå˜é‡ä¸­ã€‚è¿™ä¸ªæ–‡ä»¶ä¸­åŒ…å«äº†ä¸¤åˆ—æ•°æ®ï¼Œåˆ†åˆ«ä¸º `x` å’Œ `y` ã€‚

æˆ‘ä»¬å°†ä¼šä½¿ç”¨è¿™äº›æ•°æ®æ¥è¿›è¡Œçº¿æ€§å›žå½’çš„è®­ç»ƒä¸Žå¯è§†åŒ–ã€‚

è¯·æ³¨æ„ï¼Œåœ¨åŽç»­æœ¬notebookä¸­ä½¿ç”¨å…¶ä»–æ•°æ®çš„æ—¶å€™ï¼Œè¯·å‹¿å†æ¬¡å‘½åä¸ºdataã€‚

æ•°æ®æ¥æºï¼š

[Data on length-weight and length-length relationships, mean condition factor, and gonadosomatic index of Rutilus rutilus and Perca fluviatilis from the Ob River basin, Western Siberia - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S2352340922002785?via%3Dihub#fig0004)

```python
# è¿™ä¸€code blockç”¨æ¥importéœ€è¦çš„åº“

import pandas as pd  # ç”¨æ¥è¯»å–excelç­‰æ–‡ä»¶
import random  # ç”¨æ¥è¿›è¡Œéšæœºæ‰“ä¹±æ•°æ®
import numpy as np  # ç”¨æ¥è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œåº”å¯¹å¤šå˜é‡çº¿æ€§å›žå½’
```

```python
# è¿™ä¸€code blockç”¨æ¥è¯»å–æ•°æ®

data = pd.read_excel("data.xlsx")  # è¯»å–excelæ–‡ä»¶ï¼ˆå•å˜é‡çº¿æ€§å›žå½’â€”â€”æµ‹è¯•æ–‡ä»¶ï¼‰
```

```py
# è¿™ä¸€code blockç”¨æ¥å¯¹è¯»å–çš„æ•°æ®è¿›è¡Œä¸€äº›å¤„ç†

# ä»Žæ•°æ®æ¡†æž¶ä¸­æå–xå’Œyå€¼
x_values = data['x'].values
y_values = data['y'].values
```

## 0.å·¥å…·å‡½æ•°

åœ¨è¿™ä¸€section, æˆ‘ä»¬å°†ä¼šå®šä¹‰ä¸€äº›å·¥å…·å‡½æ•°ï¼Œä»¥ä¾¿äºŽåŽç»­çš„æ“ä½œã€‚

ç›®å½•ï¼š

1. å¯è§†åŒ–å·¥å…·å‡½æ•°
2. çº¿æ€§å›žå½’æ¨¡åž‹è®¡ç®—
3. æŸå¤±å‡½æ•°è®¡ç®—

```py
# å¯è§†åŒ–å·¥å…·å‡½æ•°
# å¯¹äºŽæ•°æ®ç‚¹ä¸Žæ‹Ÿåˆç›´çº¿çš„å¯è§†åŒ–
def plot_data_and_line(x_values, y_values, theta_0_final, theta_1_final, cost_history, title):
    """
    Plot data points and the fitted line.
    
    :param x_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param theta_0_final: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæœ€ç»ˆçš„theta_0
    :param theta_1_final: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæœ€ç»ˆçš„theta_1
    :param cost_history: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ¯ä¸€æ¬¡è¿­ä»£åŽçš„æŸå¤±å‡½æ•°å€¼
    :param title: è¿™æ˜¯ä¸€ä¸ªstringï¼Œè¡¨ç¤ºå›¾åƒçš„æ ‡é¢˜
    :return: è¿”å›žä¸€ä¸ªå›¾åƒ
    """
    import matplotlib.pyplot as plt  # ç”¨æ¥ç”»å›¾

    plt.figure(figsize=(12, 5))

    # Subplot 1: Linear Regression
    # è¿™ä¸ªsubplotç”¨æ¥ç”»å‡ºæ•°æ®ç‚¹å’Œæ‹Ÿåˆç›´çº¿
    plt.subplot(1, 2, 1)
    plt.scatter(x_values, y_values, color='blue', label='Original Data')  # è¿™é‡Œçš„scatterç”¨æ¥ç”»å‡ºæ•°æ®ç‚¹
    plt.plot(x_values, [f_theta(x, theta_0_final, theta_1_final) for x in x_values], color='red',
             label='Linear Regression')  # è¿™é‡Œçš„åˆ—è¡¨è¡¨è¾¾å¼ç”¨æ¥ç”»å‡ºæ‹Ÿåˆç›´çº¿
    plt.title(title)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True)  # æ˜¾ç¤ºç½‘æ ¼

    # Subplot 2: Cost function history
    # è¿™ä¸ªsubplotç”¨æ¥ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–
    plt.subplot(1, 2, 2)
    plt.plot(cost_history, color='green')  # è¿™é‡Œçš„plotç”¨æ¥ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–
    plt.title('Cost Function History')
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.grid(True)  # æ˜¾ç¤ºç½‘æ ¼

    plt.tight_layout()  # è°ƒæ•´å­å›¾ä¹‹é—´çš„é—´è·
    plt.show()
```

hypothesis:

$$
f_\theta(x)=\theta_0+\theta_1x
$$


```python
def f_theta(x, theta_0, theta_1):
    """
    Linear regression model.
    
    :param x: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºè¾“å…¥çš„xå€¼
    :param theta_0: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºtheta_0
    :param theta_1: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºtheta_1
    :return: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºé¢„æµ‹å€¼
    """
    return theta_0 + theta_1 * x
```

cost fuction:

$$
J(\theta_0,\theta_1)=\frac1{2N}\sum_{i=1}^N(f_\theta(x^{(i)})-y^{(i)})^2
$$

```python
def compute_cost(x_values, y_values, theta_0, theta_1):
    """
    Compute the cost function.
    
    :param x_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param theta_0: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºtheta_0
    :param theta_1: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºtheta_1
    :return: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæŸå¤±å‡½æ•°çš„å€¼
    """

    # è®¡ç®—çš„å…¬å¼ä¸ºï¼šJ(theta_0, theta_1) = 1/2N * sum((f_theta(x_i) - y_i)^2)
    N = len(x_values)
    total_error = 0
    for i in range(len(x_values)):
        total_error += (f_theta(x_values[i], theta_0, theta_1) - y_values[i]) ** 2
    return total_error / (2 * N)
```

## 1. æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• Batch Gradient Descent

repeat until convergenceï¼š

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \\
 (\text{for }j=1\text{ and }j=0)
$$

Repeat until convergence:

$$
\begin{aligned}\theta_0{:}&=\theta_0-a\frac1N\sum_{i=1}^N(f_\theta\big(x^{(i)}\big)-y^{(i)})\\\theta_1{:}&=\theta_1-a\frac1N\sum_{i=1}^N(f_\theta\big(x^{(i)}\big)-y^{(i)})x^{(i)}\end{aligned}
$$

```python
def gradient_descent(x_values, y_values, alpha=0.05, convergence_threshold=1e-8, max_iterations=10000):
    """
    Perform gradient descent to learn theta_0 and theta_1.
    
    :param x_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y_values: è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param alpha: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºå­¦ä¹ çŽ‡
    :param convergence_threshold: è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæ”¶æ•›é˜ˆå€¼
    :param max_iterations: è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºæœ€å¤§è¿­ä»£æ¬¡æ•°
    :return: è¿™æ˜¯ä¸€ä¸ªtupleï¼ŒåŒ…å«äº†theta_0, theta_1, cost_historyï¼Œåˆ†åˆ«è¡¨ç¤ºæœ€ç»ˆçš„theta_0, theta_1å’ŒæŸå¤±å‡½æ•°çš„å˜åŒ–
    """

    # è®¡ç®—å…¬å¼ä¸ºï¼š theta_j = theta_j - alpha * 1/N * sum((f_theta(x_i) - y_i) * x_i)

    theta_0 = 0  # åˆå§‹åŒ–theta_0
    theta_1 = 0  # åˆå§‹åŒ–theta_1
    N = len(x_values)  # æ ·æœ¬æ•°é‡

    cost_history = []  # ç”¨æ¥ä¿å­˜æŸå¤±å‡½æ•°çš„å˜åŒ–
    for _ in range(max_iterations):  # è¿›è¡Œè¿­ä»£
        sum_theta_0 = 0  # ç”¨æ¥è®¡ç®—theta_0çš„æ¢¯åº¦
        sum_theta_1 = 0  # ç”¨æ¥è®¡ç®—theta_1çš„æ¢¯åº¦
        for i in range(N):
            error = f_theta(x_values[i], theta_0, theta_1) - y_values[i]  # è®¡ç®—è¯¯å·®
            sum_theta_0 += error
            sum_theta_1 += error * x_values[i]
        # æ³¨æ„ï¼Œæ‰€æœ‰çš„thetaçš„æ›´æ–°éƒ½æ˜¯åœ¨åŒä¸€æ—¶åˆ»è¿›è¡Œçš„
        theta_0 -= alpha * (1 / N) * sum_theta_0
        theta_1 -= alpha * (1 / N) * sum_theta_1
        cost_history.append(compute_cost(x_values, y_values, theta_0, theta_1))  # è®¡ç®—æŸå¤±å‡½æ•°çš„å€¼

        if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < convergence_threshold:
            # å¦‚æžœæŸå¤±å‡½æ•°çš„å˜åŒ–å°äºŽæ”¶æ•›é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break

    return theta_0, theta_1, cost_history
```

```python
# è¿™ä¸€code blockç”¨æ¥è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
theta_0_final, theta_1_final, cost_history = gradient_descent(x_values, y_values)

# æ‰“å°æœ€ç»ˆçš„theta_0, theta_1, cost
theta_0_final, theta_1_final, cost_history[-1]
```

```python
# è¿™ä¸€code blockç”¨æ¥ç”»å‡ºæ•°æ®ç‚¹å’Œæ‹Ÿåˆç›´çº¿
plot_data_and_line(x_values, y_values, theta_0_final, theta_1_final, cost_history,
                   'Linear Regression using Gradient Descent')
```

![image-20231217180806773](https://cdn.jsdelivr.net/gh/Zhu-Shatong/cloudimg/img/image-20231217180806773.png)

## 2. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³• Mini Batch Gradient Descentï¼ˆåœ¨æ‰¹é‡æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼‰
$$
\begin{aligned}\theta_0&:=\theta_0-a\frac1{N_k}\sum_{i=1}^{N_k}(f_\theta\big(x^{(i)}\big)-y^{(i)})\\\theta_1&:=\theta_1-a\frac1{N_k}\sum_{i=1}^{N_k}(f_\theta\big(x^{(i)}\big)-y^{(i)})x^{(i)}\end{aligned}
$$
```python
def mini_batch_gradient_descent(x_values, y_values, batch_size=5, alpha=0.05, convergence_threshold=1e-8,
                                max_iterations=10000):
    """
    Perform mini batch gradient descent to learn theta_0 and theta_1.
    
    :param x_values:  è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y_values:  è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param batch_size:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºbatchçš„å¤§å°
    :param alpha:  è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºå­¦ä¹ çŽ‡
    :param convergence_threshold:  è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæ”¶æ•›é˜ˆå€¼
    :param max_iterations:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºæœ€å¤§è¿­ä»£æ¬¡æ•°
    :return:  è¿™æ˜¯ä¸€ä¸ªtupleï¼ŒåŒ…å«äº†theta_0, theta_1, cost_historyï¼Œåˆ†åˆ«è¡¨ç¤ºæœ€ç»ˆçš„theta_0, theta_1å’ŒæŸå¤±å‡½æ•°çš„å˜åŒ–
    """

    theta_0 = 0  # åˆå§‹åŒ–theta_0
    theta_1 = 0  # åˆå§‹åŒ–theta_1
    N = len(x_values)
    cost_history = []

    for _ in range(max_iterations):
        # å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±
        combined = list(zip(x_values, y_values))  # å°†x_valueså’Œy_valuesæ‰“åŒ…æˆä¸€ä¸ªlist
        random.shuffle(combined)  # å¯¹æ‰“åŒ…åŽçš„listè¿›è¡Œéšæœºæ‰“ä¹±
        x_values[:], y_values[:] = zip(*combined)  # å°†æ‰“ä¹±åŽçš„listè§£åŒ…èµ‹å€¼ç»™x_valueså’Œy_values

        # Mini-batch updates
        # è¿™é‡Œçš„ä»£ç ä¸Žbatch gradient descentçš„ä»£ç ç±»ä¼¼ï¼Œåªæ˜¯å¤šäº†ä¸€ä¸ªbatch_sizeçš„å‚æ•°
        # å¯¹äºŽæ¯ä¸€ä¸ªbatchï¼Œéƒ½ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œå¹¶æ›´æ–°theta_0å’Œtheta_1
        for i in range(0, N, batch_size):  # iä»Ž0å¼€å§‹ï¼Œæ¯æ¬¡å¢žåŠ batch_size
            x_batch = x_values[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 
            y_batch = y_values[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 
            sum_theta_0 = 0  # ç”¨æ¥è®¡ç®—theta_0çš„æ¢¯åº¦
            sum_theta_1 = 0  # ç”¨æ¥è®¡ç®—theta_1çš„æ¢¯åº¦
            for j in range(len(x_batch)):  # å¯¹äºŽæ¯ä¸€ä¸ªbatchä¸­çš„å…ƒç´ 
                error = f_theta(x_batch[j], theta_0, theta_1) - y_batch[j]
                sum_theta_0 += error
                sum_theta_1 += error * x_batch[j]
            theta_0 -= alpha * (1 / batch_size) * sum_theta_0
            theta_1 -= alpha * (1 / batch_size) * sum_theta_1
        cost_history.append(compute_cost(x_values, y_values, theta_0, theta_1))

        if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < convergence_threshold:
            # å¦‚æžœæŸå¤±å‡½æ•°çš„å˜åŒ–å°äºŽæ”¶æ•›é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break

    return theta_0, theta_1, cost_history
```

```python
# è¿™ä¸€code blockç”¨æ¥è°ƒç”¨ä¸Šé¢çš„å‡½æ•°

# Kå€¼çš„é€‰æ‹©éœ€è¦æˆ‘ä»¬ä¸æ–­å°è¯•ä¸Žæ¯”è¾ƒï¼Œæ¥èŽ·å–æ›´å¥½çš„æ•ˆæžœ
possible_K_values = [1, 3, 4, 5, 6, 7, 10]  # å¯èƒ½å¾—Kå€¼éœ€è¦è‡ªå·±è®¾å®šï¼Œå¯¹äºŽä¸åŒçš„æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„Kå€¼
best_K = possible_K_values[0]
lowest_cost = float('inf')
theta_0_mini_batch = 0
theta_1_mini_batch = 0
cost_history_mini_batch = []

for K in possible_K_values:  # å¯¹äºŽæ¯ä¸€ä¸ªKå€¼
    theta_0_temp, theta_1_temp, cost_history_temp = mini_batch_gradient_descent(x_values, y_values, K)
    if cost_history_temp[-1] < lowest_cost:  # å¦‚æžœæŸå¤±å‡½æ•°çš„å€¼æ›´å°
        lowest_cost = cost_history_temp[-1]
        best_K = K
        theta_0_mini_batch = theta_0_temp
        theta_1_mini_batch = theta_1_temp
        cost_history_mini_batch = cost_history_temp

best_K, theta_0_mini_batch, theta_1_mini_batch, lowest_cost
```

```python
# è¿™ä¸€code blockç”¨æ¥ç”»å‡ºæ•°æ®ç‚¹å’Œæ‹Ÿåˆç›´çº¿
plot_data_and_line(x_values, y_values, theta_0_mini_batch, theta_1_mini_batch, cost_history_mini_batch,
                   'Linear Regression using Mini Batch Gradient Descent, K= ' + str(best_K))
```

![image-20231217180900418](https://cdn.jsdelivr.net/gh/Zhu-Shatong/cloudimg/img/image-20231217180900418.png)

## 3. è‡ªé€‚åº”æ¢¯åº¦ä¸‹é™æ³• Adagradï¼ˆåœ¨å­¦ä¹ çŽ‡æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼‰
$$
\begin{aligned}\theta^{(\mathbf{t+1})}{:}=\theta^{(\mathbf{t})}-\frac{a}{\sqrt{\sum_{i=0}^{t}(g^{(i)})^2}}g^{(t)}\end{aligned}
$$
å…¶ä¸­
$$
g^{(t)}=\frac{\partial J(\theta^{(t)})}{\partial\theta}
$$
```python
# è¯·æ³¨æ„è¿™é‡Œçš„å­¦ä¹ çŽ‡ï¼Œæˆ‘å°†å®ƒè®¾å®šçš„éžå¸¸å¤§ï¼Œå¾—ç›ŠäºŽadagradçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ çŽ‡
# å¦‚æžœå°†å­¦ä¹ çŽ‡è®¾å®šè¿‡å°ï¼Œä¼šå¯¼è‡´adagradæ— æ³•æ”¶æ•›ï¼Œæ•ˆæžœè¾ƒå·®
# æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦alphaä¹Ÿéœ€è¦ä¸æ–­å°è¯•ä¸Žæ¯”è¾ƒï¼Œæ¥èŽ·å–æ›´å¥½çš„æ•ˆæžœ
def adagrad_mini_batch_gradient_descent(x_values, y_values, batch_size=5, alpha=3, convergence_threshold=1e-8,
                                        max_iterations=10000):
    """
    Perform mini batch gradient descent with adaptive learning rate.
    
    :param x_values:  è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y_values:  è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param batch_size:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºbatchçš„å¤§å°
    :param alpha:   è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºå­¦ä¹ çŽ‡
    :param convergence_threshold:  è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæ”¶æ•›é˜ˆå€¼
    :param max_iterations:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºæœ€å¤§è¿­ä»£æ¬¡æ•°
    :return:    è¿™æ˜¯ä¸€ä¸ªtupleï¼ŒåŒ…å«äº†theta_0, theta_1, cost_historyï¼Œåˆ†åˆ«è¡¨ç¤ºæœ€ç»ˆçš„theta_0, theta_1å’ŒæŸå¤±å‡½æ•°çš„å˜åŒ–
    """

    theta_0 = 0  # åˆå§‹åŒ–theta_0
    theta_1 = 0  # åˆå§‹åŒ–theta_1
    N = len(x_values)
    cost_history = []

    # åˆå§‹åŒ–sum_squared_gradientsï¼Œè¿™æ˜¯ç”¨æ¥è®¡ç®—å­¦ä¹ çŽ‡çš„
    sum_squared_gradients_0 = 0.0001  # è¾ƒå°çš„å€¼ä»¥é¿å…è¢«é›¶é™¤
    sum_squared_gradients_1 = 0.0001

    for _ in range(max_iterations):
        # å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±
        combined = list(zip(x_values, y_values))  # å°†x_valueså’Œy_valuesæ‰“åŒ…æˆä¸€ä¸ªlist
        random.shuffle(combined)  # å¯¹æ‰“åŒ…åŽçš„listè¿›è¡Œéšæœºæ‰“ä¹±
        x_values[:], y_values[:] = zip(*combined)  # å°†æ‰“ä¹±åŽçš„listè§£åŒ…èµ‹å€¼ç»™x_valueså’Œy_values

        # Mini-batch updates
        # è¿™é‡Œçš„ä»£ç ä¸Žbatch gradient descentçš„ä»£ç ç±»ä¼¼ï¼Œåªæ˜¯å¤šäº†ä¸€ä¸ªbatch_sizeçš„å‚æ•°
        for i in range(0, N, batch_size):
            x_batch = x_values[i:i + batch_size]
            y_batch = y_values[i:i + batch_size]
            sum_theta_0 = 0
            sum_theta_1 = 0
            for j in range(len(x_batch)):
                error = f_theta(x_batch[j], theta_0, theta_1) - y_batch[j]
                sum_theta_0 += error
                sum_theta_1 += error * x_batch[j]

            # è®¡ç®—æ¢¯åº¦
            # è®¡ç®—å…¬å¼ä¸ºï¼š theta_j = theta_j - alpha / (sum_squared_gradients_j ** 0.5) * 1/N * sum((f_theta(x_i) - y_i) * x_i)
            gradient_0 = (1 / batch_size) * sum_theta_0  # è®¡ç®—theta_0çš„æ¢¯åº¦
            gradient_1 = (1 / batch_size) * sum_theta_1  # è®¡ç®—theta_1çš„æ¢¯åº¦

            sum_squared_gradients_0 += gradient_0 ** 2  # æ›´æ–°sum_squared_gradients_0
            sum_squared_gradients_1 += gradient_1 ** 2  # æ›´æ–°sum_squared_gradients_1

            adaptive_alpha_0 = alpha / (sum_squared_gradients_0 ** 0.5)  # è®¡ç®—theta_0çš„å­¦ä¹ çŽ‡
            adaptive_alpha_1 = alpha / (sum_squared_gradients_1 ** 0.5)  # è®¡ç®—theta_1çš„å­¦ä¹ çŽ‡

            theta_0 -= adaptive_alpha_0 * gradient_0  # æ›´æ–°theta_0
            theta_1 -= adaptive_alpha_1 * gradient_1  # æ›´æ–°theta_1

        cost_history.append(compute_cost(x_values, y_values, theta_0, theta_1))

        if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < convergence_threshold:
            # å¦‚æžœæŸå¤±å‡½æ•°çš„å˜åŒ–å°äºŽæ”¶æ•›é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break

    return theta_0, theta_1, cost_history
```

```python
# è¿™ä¸€code blockç”¨æ¥è°ƒç”¨ä¸Šé¢çš„å‡½æ•°

# Kå€¼çš„é€‰æ‹©éœ€è¦æˆ‘ä»¬ä¸æ–­å°è¯•ä¸Žæ¯”è¾ƒï¼Œæ¥èŽ·å–æ›´å¥½çš„æ•ˆæžœ
possible_K_values = [3, 4, 5, 6, 7, 10]  # å¯èƒ½å¾—Kå€¼éœ€è¦è‡ªå·±è®¾å®šï¼Œå¯¹äºŽä¸åŒçš„æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦ä¸åŒçš„Kå€¼
best_K = possible_K_values[0]
lowest_cost = float('inf')
theta_0_adaptive = 0
theta_1_adaptive = 0
cost_history_adaptive = []

for K in possible_K_values:  # å¯¹äºŽæ¯ä¸€ä¸ªKå€¼
    theta_0_temp, theta_1_temp, cost_history_temp = adagrad_mini_batch_gradient_descent(x_values, y_values, K)
    if cost_history_temp[-1] < lowest_cost:
        lowest_cost = cost_history_temp[-1]
        best_K = K
        theta_0_adaptive = theta_0_temp
        theta_1_adaptive = theta_1_temp
        cost_history_adaptive = cost_history_temp

best_K, theta_0_adaptive, theta_1_adaptive, cost_history_adaptive[-1]
```

```python
# è¿™ä¸€code blockç”¨æ¥ç”»å‡ºæ•°æ®ç‚¹å’Œæ‹Ÿåˆç›´çº¿
plot_data_and_line(x_values, y_values, theta_0_adaptive, theta_1_adaptive, cost_history_adaptive,
                   'Linear Regression using adagrad mini batch gradient descent, K= ' + str(best_K))
```

![image-20231217180953538](https://cdn.jsdelivr.net/gh/Zhu-Shatong/cloudimg/img/image-20231217180953538.png)

## 4. å¤šå˜é‡çº¿æ€§å›žå½’ Multivariate Linear Regressionï¼ˆåœ¨ç‰¹å¾æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼Œæ‹“å±•åˆ°å¤šä¸ªç‰¹å¾ï¼‰
$$
f_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n
$$

$$
J(\theta_0,\theta_1,...\theta_n)=\frac1{2N}\sum_{i=1}^N(f_\theta(x^{(i)})-y^{(i)})^2 
$$

```python
def multivariate_gradient_descent(X, y, batch_size=5, alpha=3, convergence_threshold=1e-8, max_iterations=10000):
    """
    Perform mini batch gradient descent with adaptive learning rate for multivariate linear regression.
    
    :param X:  è¿™æ˜¯ä¸€ä¸ªçŸ©é˜µï¼ŒåŒ…å«äº†æ‰€æœ‰çš„xå€¼
    :param y:  è¿™æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«äº†æ‰€æœ‰çš„yå€¼
    :param batch_size:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºbatchçš„å¤§å°
    :param alpha:  è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºå­¦ä¹ çŽ‡
    :param convergence_threshold:  è¿™æ˜¯ä¸€ä¸ªfloatï¼Œè¡¨ç¤ºæ”¶æ•›é˜ˆå€¼
    :param max_iterations:  è¿™æ˜¯ä¸€ä¸ªintï¼Œè¡¨ç¤ºæœ€å¤§è¿­ä»£æ¬¡æ•°
    :return:  è¿™æ˜¯ä¸€ä¸ªtupleï¼ŒåŒ…å«äº†theta, cost_historyï¼Œåˆ†åˆ«è¡¨ç¤ºæœ€ç»ˆçš„thetaå’ŒæŸå¤±å‡½æ•°çš„å˜åŒ–ï¼Œthetaæ˜¯ä¸€ä¸ªlist
    """
    m, n = X.shape  # mæ˜¯æ ·æœ¬æ•°é‡ï¼Œnæ˜¯ç‰¹å¾æ•°é‡
    theta = np.zeros(n + 1)  # n+1 thetas åŒ…å« theta_0
    X = np.hstack((np.ones((m, 1)), X))  # åœ¨Xå‰é¢åŠ ä¸€åˆ—1ï¼Œç”¨æ¥è®¡ç®—theta_0
    cost_history = []
    sum_squared_gradients = np.zeros(n + 1) + 0.0001  # è¾ƒå°çš„å€¼ä»¥é¿å…è¢«é›¶é™¤

    for _ in range(max_iterations):
        # å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±
        indices = np.arange(m)  # ç”Ÿæˆä¸€ä¸ª0åˆ°m-1çš„list
        np.random.shuffle(indices)  # å¯¹listè¿›è¡Œéšæœºæ‰“ä¹±
        X = X[indices]  # ç”¨æ‰“ä¹±åŽçš„listå¯¹Xè¿›è¡Œé‡æ–°æŽ’åº
        y = y[indices]  # ç”¨æ‰“ä¹±åŽçš„listå¯¹yè¿›è¡Œé‡æ–°æŽ’åº

        # Mini-batch updates
        for i in range(0, m, batch_size):  # iä»Ž0å¼€å§‹ï¼Œæ¯æ¬¡å¢žåŠ batch_size
            X_batch = X[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 
            y_batch = y[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 

            # æ¢¯åº¦è®¡ç®—å…¬å¼ä¸ºï¼š theta_j = theta_j - alpha / (sum_squared_gradients_j ** 0.5) * 1/N * sum((f_theta(x_i) - y_i) * x_i) 
            gradient = (1 / batch_size) * X_batch.T.dot(X_batch.dot(theta) - y_batch)  # è®¡ç®—æ¢¯åº¦
            sum_squared_gradients += gradient ** 2  # æ›´æ–°sum_squared_gradients
            adaptive_alpha = alpha / np.sqrt(sum_squared_gradients)  # è®¡ç®—å­¦ä¹ çŽ‡
            theta -= adaptive_alpha * gradient  # æ›´æ–°theta

        cost = (1 / (2 * m)) * np.sum((X.dot(theta) - y) ** 2)  # è®¡ç®—æŸå¤±å‡½æ•°çš„å€¼
        cost_history.append(cost)

        if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < convergence_threshold:
            # å¦‚æžœæŸå¤±å‡½æ•°çš„å˜åŒ–å°äºŽæ”¶æ•›é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break

    return theta, cost_history
```

```python
# è¿™ä¸€code blockç”¨æ¥è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
# è¯·æ³¨æ„ï¼Œè¿™é‡Œçš„æ•°æ®é›†æ˜¯å¤šå˜é‡çº¿æ€§å›žå½’çš„æ•°æ®é›†
X_matrix = data[['x']].values
y_vector = data['y'].values
# best_K å·²ç»åœ¨ä¸Šé¢çš„ä»£ç ä¸­è¢«èµ‹å€¼
theta_multivariate, cost_history_multivariate = multivariate_gradient_descent(X_matrix, y_vector, best_K)

theta_multivariate, cost_history_multivariate[-1]
```

## 5. L1æ­£åˆ™åŒ– L1 Regularizationï¼ˆåœ¨æ­£åˆ™åŒ–æ–¹é¢è¿›è¡Œäº†æ”¹è¿›ï¼‰

[çº¿æ€§å›žå½’â€”â€”lassoå›žå½’å’Œå²­å›žå½’ï¼ˆridge regressionï¼‰ - wuliytTaotao - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/wuliytTaotao/p/10837533.html)

```python
def lasso_gradient_descent(X, y, batch_size=5, lambda_=0.1, alpha=3, convergence_threshold=1e-8, max_iterations=10000):
    """Perform mini batch gradient descent with adaptive learning rate and L1 regularization for multivariate linear regression."""
    m, n = X.shape  # mæ˜¯æ ·æœ¬æ•°é‡ï¼Œnæ˜¯ç‰¹å¾æ•°é‡
    theta = np.zeros(n + 1)  # n+1 thetas åŒ…å« theta_0
    X = np.hstack((np.ones((m, 1)), X))  # åœ¨Xå‰é¢åŠ ä¸€åˆ—1ï¼Œç”¨æ¥è®¡ç®—theta_0
    cost_history = []
    sum_squared_gradients = np.zeros(n + 1) + 0.0001  # è¾ƒå°çš„å€¼ä»¥é¿å…è¢«é›¶é™¤

    for _ in range(max_iterations):
        # å¯¹æ•°æ®è¿›è¡Œéšæœºæ‰“ä¹±
        indices = np.arange(m)  # ç”Ÿæˆä¸€ä¸ª0åˆ°m-1çš„list
        np.random.shuffle(indices)  # å¯¹listè¿›è¡Œéšæœºæ‰“ä¹±
        X = X[indices]  # ç”¨æ‰“ä¹±åŽçš„listå¯¹Xè¿›è¡Œé‡æ–°æŽ’åº
        y = y[indices]  # ç”¨æ‰“ä¹±åŽçš„listå¯¹yè¿›è¡Œé‡æ–°æŽ’åº

        # Mini-batch updates
        for i in range(0, m, batch_size):  # iä»Ž0å¼€å§‹ï¼Œæ¯æ¬¡å¢žåŠ batch_size
            X_batch = X[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 
            y_batch = y[i:i + batch_size]  # ä»Žiå¼€å§‹ï¼Œå–batch_sizeä¸ªå…ƒç´ 

            # Compute gradient (including L1 penalty for j > 0)
            gradient = (1 / batch_size) * X_batch.T.dot(X_batch.dot(theta) - y_batch)  # è®¡ç®—æ¢¯åº¦
            gradient[1:] += lambda_ * np.sign(theta[1:])  # å¯¹é™¤theta_0å¤–çš„æ‰€æœ‰thetaæ·»åŠ L1æ­£åˆ™åŒ–

            sum_squared_gradients += gradient ** 2  # æ›´æ–°sum_squared_gradients
            adaptive_alpha = alpha / np.sqrt(sum_squared_gradients)  # è®¡ç®—å­¦ä¹ çŽ‡
            theta -= adaptive_alpha * gradient  # æ›´æ–°theta

        # Compute cost (including L1 penalty for j > 0)
        cost = (1 / (2 * m)) * np.sum((X.dot(theta) - y) ** 2) + lambda_ * np.sum(np.abs(theta[1:]))
        cost_history.append(cost)

        if len(cost_history) > 1 and abs(cost_history[-1] - cost_history[-2]) < convergence_threshold:
            # å¦‚æžœæŸå¤±å‡½æ•°çš„å˜åŒ–å°äºŽæ”¶æ•›é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£
            break

    return theta, cost_history
```

å¦‚ä½•é€‰æ‹©lambdaï¼Ÿ

```python
def determine_best_lambda(X, y, lambdas, num_folds=5, **kwargs):
    """Determine the best lambda using K-fold cross validation."""
    from sklearn.model_selection import KFold  # æ­¤å¤„ä½¿ç”¨sklearnä¸­çš„KFoldå‡½æ•°ï¼Œç”¨æ¥è¿›è¡Œäº¤å‰éªŒè¯ï¼Œä¸Žçº¿æ€§å›žå½’æ— å…³
    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)  # ç”Ÿæˆäº¤å‰éªŒè¯çš„æ•°æ®,42æ˜¯éšæœºç§å­
    average_errors = []  # ç”¨æ¥ä¿å­˜æ¯ä¸€ä¸ªlambdaçš„å¹³å‡è¯¯å·®

    for lambda_ in lambdas:  # å¯¹äºŽæ¯ä¸€ä¸ªlambda
        fold_errors = []  # ç”¨æ¥ä¿å­˜æ¯ä¸€æŠ˜çš„è¯¯å·®

        for train_index, val_index in kf.split(X):
            X_train, X_val = X[train_index], X[val_index]  # ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
            y_train, y_val = y[train_index], y[val_index]  # ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†

            theta, _ = lasso_gradient_descent(X_train, y_train, lambda_=lambda_, **kwargs)  # è®­ç»ƒæ¨¡åž‹

            # Compute validation error
            y_pred = np.hstack((np.ones((X_val.shape[0], 1)), X_val)).dot(theta)  # è®¡ç®—é¢„æµ‹å€¼
            error = (1 / (2 * X_val.shape[0])) * np.sum((y_pred - y_val) ** 2)  # è®¡ç®—è¯¯å·®
            fold_errors.append(error)

        average_errors.append(np.mean(fold_errors))

    best_lambda = lambdas[np.argmin(average_errors)]  # é€‰æ‹©å¹³å‡è¯¯å·®æœ€å°çš„lambda
    return best_lambda, average_errors
```

```python
# Lambda values to test
lambdas = [0, 0.001, 0.01, 0.1, 1, 10]

best_lambda, average_errors = determine_best_lambda(X_matrix, y_vector, lambdas)
best_lambda, average_errors
```

```python
# Apply the multivariate gradient descent (using the single feature we have for this dataset)
X_matrix = data[['x']].values
y_vector = data['y'].values
theta_lasso, cost_history_lasso = lasso_gradient_descent(X_matrix, y_vector, best_K, best_lambda)

theta_lasso, cost_history_lasso[-1]
```

